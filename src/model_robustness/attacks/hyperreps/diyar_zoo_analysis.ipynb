{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520dd8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device='cpu'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from einops import repeat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from shrp.models.def_net import NNmodule\n",
    "#from shrp.models.def_AE_module import AEModule\n",
    "from shrp.datasets.dataset_tokens import DatasetTokens\n",
    "from shrp.git_re_basin.git_re_basin import resnet18_permutation_spec\n",
    "from shrp.datasets.def_FastTensorDataLoader import FastTensorDataLoader\n",
    "\n",
    "from shrp.git_re_basin.git_re_basin import (\n",
    "    resnet18_permutation_spec,\n",
    "    zoo_cnn_large_permutation_spec,\n",
    "    zoo_cnn_permutation_spec)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3f901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select clean and trig zoos\n",
    "size = 'small' # 'small' or 'large' CNN\n",
    "path_clean = Path('/ds2/model_zoos/zoos_v2/MNIST/tune_zoo_mnist_uniform/')\n",
    "path_trig = Path('/ds2/model_zoos/zoos_backdoors/MNIST/tune_zoo_mnist_uniform_nc_1_p_25/')\n",
    "\n",
    "# select hyperrep for embedding\n",
    "path_hyper = Path('/netscratch2/kschuerholt/code/shrp/experiments/03_trojai/02_mnist/tune/trojai_mnist_clean_v1/AE_trainable_57787_00003_3_ae_d_model=256,ae_lat_dim=32,training_view_2_canon=False_2023-06-22_22-20-41')\n",
    "hyper_chkpt = 100 # checkpoit of AE to load\n",
    "\n",
    "# select location to save data to (None skips this)\n",
    "path_eval_record = None # NOT YET IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb1af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure new datasets zoo\n",
    "if size == 'small':\n",
    "    permutation_spec = zoo_cnn_permutation_spec() # small CNN\n",
    "else:\n",
    "    permutation_spec = zoo_cnn_large_permutation_spec() # large CNN\n",
    "\n",
    "## clean specific\n",
    "result_key_list_clean = [\"test_acc\", \"training_iteration\"]\n",
    "config_key_list = []\n",
    "property_keys_clean = {\n",
    "    \"result_keys\": result_key_list_clean,\n",
    "    \"config_keys\": config_key_list,\n",
    "}\n",
    "\n",
    "## trig specific\n",
    "result_key_list_trig = [\"triggered/test_acc\", \"clean/test_acc\", \"training_iteration\"]\n",
    "property_keys_trig = {\n",
    "    \"result_keys\": result_key_list_trig,\n",
    "    \"config_keys\": config_key_list,\n",
    "}\n",
    "tokensize = 0 # discover tokensize from dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6828516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config to set how datasets should be downloaded. set in single place to ensure \n",
    "dataset_config = {\n",
    "    'epoch_lst' : 50,\n",
    "    'mode' : \"vector\",\n",
    "    'permutation_spec' : permutation_spec,\n",
    "    'map_to_canonical' : False,\n",
    "    'standardize' : \"l2_ind\",\n",
    "    'ds_split' : [0.7, 0.15, 0.15],  \n",
    "    'max_samples' : 1000,\n",
    "    'weight_threshold' : 15000,\n",
    "    'precision' : \"32\",\n",
    "    'filter_function' : None,  # gets sample path as argument and returns True if model needs to be filtered out\n",
    "    'num_threads' : 8,\n",
    "    'shuffle_path' : True,\n",
    "    'verbosity' : 3,\n",
    "    'getitem' : \"tokens+props\",\n",
    "    'ignore_bn' : False,\n",
    "    'tokensize' : tokensize\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85940b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 17:14:51,670\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# load triggered zoo -- train, test, val\n",
    "trainset_clean = DatasetTokens(root=[path_clean.absolute()], train_val_test='train', property_keys=property_keys_clean, **dataset_config)\n",
    "testset_clean = DatasetTokens(root=[path_clean.absolute()], train_val_test='test',  property_keys=property_keys_clean, **dataset_config)\n",
    "valset_clean = DatasetTokens(root=[path_clean.absolute()], train_val_test='val',  property_keys=property_keys_clean, **dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d2fffc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0345, -0.0816, -0.0300,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0266, -0.0376, -0.0218,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0012,  0.0384, -0.0352,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.0788,  0.0630,  0.0752,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0399,  0.0349, -0.0091,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0515,  0.0057,  0.0345,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False]]),\n",
       " tensor([[ 0,  0,  0],\n",
       "         [ 1,  0,  1],\n",
       "         [ 2,  0,  2],\n",
       "         [ 3,  0,  3],\n",
       "         [ 4,  0,  4],\n",
       "         [ 5,  0,  5],\n",
       "         [ 6,  0,  6],\n",
       "         [ 7,  0,  7],\n",
       "         [ 8,  1,  0],\n",
       "         [ 9,  1,  1],\n",
       "         [10,  1,  2],\n",
       "         [11,  1,  3],\n",
       "         [12,  1,  4],\n",
       "         [13,  1,  5],\n",
       "         [14,  2,  0],\n",
       "         [15,  2,  1],\n",
       "         [16,  2,  2],\n",
       "         [17,  2,  3],\n",
       "         [18,  3,  0],\n",
       "         [19,  3,  1],\n",
       "         [20,  3,  2],\n",
       "         [21,  3,  3],\n",
       "         [22,  3,  4],\n",
       "         [23,  3,  5],\n",
       "         [24,  3,  6],\n",
       "         [25,  3,  7],\n",
       "         [26,  3,  8],\n",
       "         [27,  3,  9],\n",
       "         [28,  3, 10],\n",
       "         [29,  3, 11],\n",
       "         [30,  3, 12],\n",
       "         [31,  3, 13],\n",
       "         [32,  3, 14],\n",
       "         [33,  3, 15],\n",
       "         [34,  3, 16],\n",
       "         [35,  3, 17],\n",
       "         [36,  3, 18],\n",
       "         [37,  3, 19],\n",
       "         [38,  4,  0],\n",
       "         [39,  4,  1],\n",
       "         [40,  4,  2],\n",
       "         [41,  4,  3],\n",
       "         [42,  4,  4],\n",
       "         [43,  4,  5],\n",
       "         [44,  4,  6],\n",
       "         [45,  4,  7],\n",
       "         [46,  4,  8],\n",
       "         [47,  4,  9]], dtype=torch.int32),\n",
       " tensor([ 0.9231, 50.0000]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "122df200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 12:09:16,317\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:02<00:00, 33.03it/s]\n",
      "70it [00:00, 464.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 5063.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 1293.40it/s]\n",
      "2023-08-16 12:09:28,160\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  6.96it/s]\n",
      "15it [00:00, 472.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 4635.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 1322.23it/s]\n",
      "2023-08-16 12:09:39,459\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  7.09it/s]\n",
      "15it [00:00, 544.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 4539.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 1382.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# load triggered zoo -- train, test, val\n",
    "trainset_trig = DatasetTokens(root=[path_trig.absolute()], train_val_test='train', property_keys=property_keys_trig, **dataset_config)\n",
    "testset_trig = DatasetTokens(root=[path_trig.absolute()], train_val_test='test',  property_keys=property_keys_trig, **dataset_config)\n",
    "valset_trig = DatasetTokens(root=[path_trig.absolute()], train_val_test='val',  property_keys=property_keys_trig, **dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f52eb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: use simclr NT_Xent loss\n",
      "Running single-gpu. send model to device: cpu\n",
      "++++++ USE AUTOMATIC MIXED PRECISION +++++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_hyper = json.load(path_hyper.joinpath('params.json').open('r'))\n",
    "config_hyper['device']=device\n",
    "module = AEModule(config_hyper).to(device)\n",
    "checkpoint = torch.load(path_hyper.joinpath(f'checkpoint_{str(hyper_chkpt).zfill(6)}/state.pt'), map_location=device)\n",
    "\n",
    "# fixing model checkpoint keys if necessary\n",
    "if '_orig_mod.' in next(iter(checkpoint['model'].keys())):\n",
    "    checkpoint['model'] = { k[k.find('.')+1:] : v for k, v in checkpoint['model'].items()}\n",
    "    \n",
    "module.model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2368d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embeds(dataset, model, device, dataset_config=dataset_config):\n",
    "    w_ds, _   = dataset.__get_weights__()\n",
    "    pos_ds = torch.stack(dataset.pos) if dataset_config['mode'] == 'tokenize' else repeat(dataset.positions, \"n d -> b n d\", b=w_ds.shape[0])\n",
    "    z_ds   = model.forward_encoder(w_ds.to(device),pos_ds.to(device))\n",
    "    return w_ds, pos_ds, z_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "554e90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train_clean, pos_train_clean, z_train_clean = map_embeds(trainset_clean, module, device)\n",
    "w_test_clean, pos_test_clean, z_test_clean = map_embeds(testset_clean, module, device)\n",
    "w_val_clean, pos_val_clean, z_val_clean = map_embeds(valset_clean, module, device)\n",
    "\n",
    "w_train_trig, pos_train_trig, z_train_trig = map_embeds(trainset_trig, module, device)\n",
    "w_test_trig, pos_test_trig, z_test_trig = map_embeds(testset_trig, module, device)\n",
    "w_val_trig, pos_val_trig, z_val_trig = map_embeds(valset_trig, module, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b13342bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train_clean[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2f24874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([770, 1536])\n",
      "torch.Size([165, 1536])\n",
      "torch.Size([165, 1536])\n",
      "torch.Size([770, 9648])\n",
      "torch.Size([165, 9648])\n",
      "torch.Size([165, 9648])\n",
      "torch.Size([770])\n",
      "torch.Size([165])\n",
      "torch.Size([165])\n"
     ]
    }
   ],
   "source": [
    "# generate labels: poisoned, not-poisoend\n",
    "fraction = 1.0\n",
    "idx_end_train = int(z_train_trig.shape[0]*fraction)\n",
    "idx_end_test = int(z_test_trig.shape[0]*fraction)\n",
    "idx_end_val = int(z_val_trig.shape[0]*fraction)\n",
    "\n",
    "z_tr = torch.cat([z_train_clean.flatten(start_dim=1),z_train_trig[:idx_end_train].flatten(start_dim=1)])\n",
    "print(z_tr.shape)\n",
    "z_te = torch.cat([z_test_clean.flatten(start_dim=1),z_test_trig[:idx_end_test].flatten(start_dim=1)])\n",
    "print(z_te.shape)\n",
    "z_vl = torch.cat([z_val_clean.flatten(start_dim=1),z_val_trig[:idx_end_val].flatten(start_dim=1)])\n",
    "print(z_vl.shape)\n",
    "\n",
    "# generate labels: poisoned, not-poisoend\n",
    "w_tr = torch.cat([w_train_clean.flatten(start_dim=1),w_train_trig[:idx_end_train].flatten(start_dim=1)])\n",
    "print(w_tr.shape)\n",
    "w_te = torch.cat([w_test_clean.flatten(start_dim=1),w_test_trig[:idx_end_test].flatten(start_dim=1)])\n",
    "print(w_te.shape)\n",
    "w_vl = torch.cat([w_val_clean.flatten(start_dim=1),w_val_trig[:idx_end_val].flatten(start_dim=1)])\n",
    "print(w_vl.shape)\n",
    "\n",
    "\n",
    "# concat\n",
    "p_tr = torch.cat([torch.zeros(z_train_clean.shape[0]),torch.ones(z_train_trig[:idx_end_train].shape[0])])\n",
    "print(p_tr.shape)\n",
    "p_te = torch.cat([torch.zeros(z_test_clean.shape[0]),torch.ones(z_test_trig[:idx_end_test].shape[0])])\n",
    "print(p_te.shape)\n",
    "p_vl = torch.cat([torch.zeros(z_val_clean.shape[0]),torch.ones(z_val_trig[:idx_end_val].shape[0])])\n",
    "print(p_vl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58ea5d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f51f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_poisoned(z_train, z_val, z_test, prop_train, prop_test, prop_val, verbose=True):\n",
    "    ## get list of classes and class labels\n",
    "    # cast to float and push to cpu. back to gpu will be done at dataloader time\n",
    "    z_train = z_train.float().to(torch.device(\"cpu\"))\n",
    "    z_test = z_test.float().to(torch.device(\"cpu\"))\n",
    "    if z_val:\n",
    "        z_val = z_val.float().to(torch.device(\"cpu\"))\n",
    "        \n",
    "    # train set classes\n",
    "    classes = list(np.unique(prop_train))\n",
    "    no_classes = len(classes)\n",
    "    # assert all classes are in trainset\n",
    "    classes_test = list(np.unique(prop_test))\n",
    "    assert set(classes_test).issubset(set(classes)), \"test set contains classes which are not in train set\"\n",
    "    if prop_val:\n",
    "        classes_val = list(np.unique(prop_val))\n",
    "        assert set(classes_val).issubset(set(classes)), \"val set contains classes which are not in train set\"\n",
    "\n",
    "    # one hot encoding\n",
    "    labels_train = torch.tensor([float(classes.index(vdx)) for idx, vdx in enumerate(prop_train)]).long()\n",
    "    labels_test = torch.tensor([float(classes.index(vdx)) for idx, vdx in enumerate(prop_test)]).long()\n",
    "    if prop_val:\n",
    "        labels_val = torch.tensor([float(classes.index(vdx)) for idx, vdx in enumerate(prop_val)]).long()\n",
    "\n",
    "    ## dataset\n",
    "    # train\n",
    "    trainset = torch.utils.data.TensorDataset(z_train, labels_train)\n",
    "    trainloader = FastTensorDataLoader(trainset, batch_size=10, shuffle=True)\n",
    "    # test\n",
    "    testset = torch.utils.data.TensorDataset(z_test, labels_test)\n",
    "    testloader = FastTensorDataLoader(testset, batch_size=10, shuffle=True)\n",
    "    # val\n",
    "    if prop_val:\n",
    "        valset = torch.utils.data.TensorDataset(z_val, labels_val)\n",
    "        valloader = FastTensorDataLoader(valset, batch_size=10, shuffle=True)\n",
    "\n",
    "    ## create model\n",
    "    # configure model\n",
    "    config = {}\n",
    "    config[\"model::type\"] = \"MLP\"\n",
    "    config[\"model::h_dim\"] = [100,100,100]\n",
    "    # config[\"model::h_dim\"] = []  # no hidden layers\n",
    "    config[\"model::i_dim\"] = z_train.shape[1]\n",
    "    config[\"model::o_dim\"] = no_classes\n",
    "    config[\"model::init_type\"] = \"kaiming_normal\"\n",
    "    config[\"model::nlin\"] = \"tanh\"\n",
    "    config[\"model::dropout\"] = 0\n",
    "    config[\"model::use_bias\"] = True\n",
    "    config[\"optim::optimizer\"] = \"adam\"\n",
    "    config[\"optim::lr\"] = 1e-4\n",
    "    config[\"optim::wd\"] = 1e-6\n",
    "    config[\"training::task\"] = \"classification\"\n",
    "    config[\"training::batchsize\"] = 5\n",
    "    config[\"training::start_epoch\"] = 1\n",
    "    config[\"training::epochs_train\"] = 15\n",
    "    config[\"training::val_epochs\"] = 10\n",
    "    config[\"training::output_epoch\"] = 10\n",
    "    config[\"training::idx_out\"] = 10\n",
    "    config[\"training::checkpoint_dir\"] = None\n",
    "    config[\"training::tensorboard_dir\"] = None\n",
    "    config[\"seed\"] = 42\n",
    "    config[\"training::trainloader\"] = trainloader\n",
    "    config[\"training::testloader\"] = testloader\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    MLP = NNmodule(config, cuda=cuda, verbosity=0)\n",
    "    ## start training loop\n",
    "    accs = {k : {'train': 0, 'test': 0, 'val': 0} for k in range(0, config[\"training::epochs_train\"])}\n",
    "    losses = {k : {'train': 0, 'test': 0, 'val': 0} for k in range(0, config[\"training::epochs_train\"])}\n",
    "    \n",
    "    for idx in range(config[\"training::epochs_train\"]):\n",
    "        loss_train, acc_train = MLP.train_epoch(trainloader, epoch=-1)\n",
    "        loss_test, acc_test = MLP.test_epoch(testloader, epoch=-1)\n",
    "        if prop_val:\n",
    "            loss_val, acc_val = MLP.test_epoch(valloader, epoch=-1)\n",
    "        else:\n",
    "            loss_val, acc_val = None, None\n",
    "        accs[idx]['train'] = acc_train\n",
    "        accs[idx]['test'] = acc_test\n",
    "        accs[idx]['val'] = acc_val\n",
    "        losses[idx]['train'] = loss_train\n",
    "        losses[idx]['test'] = loss_test\n",
    "        losses[idx]['val'] = loss_val\n",
    "        if verbose: print(f'{idx} - train: {acc_train*100:3.3f} - val {acc_val*100:3.3f} - test {acc_test*100:3.3f}')\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_asr(z_train, z_val, z_test, prop_train, prop_test, prop_val, verbose=True):\n",
    "    ## get list of classes and class labels\n",
    "    # cast to float and push to cpu. back to gpu will be done at dataloader time\n",
    "    z_train = z_train.float().to(torch.device(\"cpu\"))\n",
    "    z_test = z_test.float().to(torch.device(\"cpu\"))\n",
    "    if z_val:\n",
    "        z_val = z_val.float().to(torch.device(\"cpu\"))\n",
    "        \n",
    "    # train set classes\n",
    "    classes = list(np.unique(prop_train))\n",
    "    no_classes = len(classes)\n",
    "    # assert all classes are in trainset\n",
    "    classes_test = list(np.unique(prop_test))\n",
    "    assert set(classes_test).issubset(set(classes)), \"test set contains classes which are not in train set\"\n",
    "    if prop_val:\n",
    "        classes_val = list(np.unique(prop_val))\n",
    "        assert set(classes_val).issubset(set(classes)), \"val set contains classes which are not in train set\"\n",
    "\n",
    "    # one hot encoding\n",
    "    labels_train = torch.tensor([float(classes.index(vdx)) for idx, vdx in enumerate(prop_train)]).long()\n",
    "    labels_test = torch.tensor([float(classes.index(vdx)) for idx, vdx in enumerate(prop_test)]).long()\n",
    "    if prop_val:\n",
    "        labels_val = torch.tensor([float(classes.index(vdx)) for idx, vdx in enumerate(prop_val)]).long()\n",
    "\n",
    "    ## dataset\n",
    "    # train\n",
    "    trainset = torch.utils.data.TensorDataset(z_train, labels_train)\n",
    "    trainloader = FastTensorDataLoader(trainset, batch_size=10, shuffle=True)\n",
    "    # test\n",
    "    testset = torch.utils.data.TensorDataset(z_test, labels_test)\n",
    "    testloader = FastTensorDataLoader(testset, batch_size=10, shuffle=True)\n",
    "    # val\n",
    "    if prop_val:\n",
    "        valset = torch.utils.data.TensorDataset(z_val, labels_val)\n",
    "        valloader = FastTensorDataLoader(valset, batch_size=10, shuffle=True)\n",
    "\n",
    "    ## create model\n",
    "    # configure model\n",
    "    config = {}\n",
    "    config[\"model::type\"] = \"MLP\"\n",
    "    config[\"model::h_dim\"] = [100,100,100]\n",
    "    # config[\"model::h_dim\"] = []  # no hidden layers\n",
    "    config[\"model::i_dim\"] = z_train.shape[1]\n",
    "    config[\"model::o_dim\"] = no_classes\n",
    "    config[\"model::init_type\"] = \"kaiming_normal\"\n",
    "    config[\"model::nlin\"] = \"tanh\"\n",
    "    config[\"model::dropout\"] = 0\n",
    "    config[\"model::use_bias\"] = True\n",
    "    config[\"optim::optimizer\"] = \"adam\"\n",
    "    config[\"optim::lr\"] = 1e-4\n",
    "    config[\"optim::wd\"] = 1e-6\n",
    "    config[\"training::task\"] = \"classification\"\n",
    "    config[\"training::batchsize\"] = 5\n",
    "    config[\"training::start_epoch\"] = 1\n",
    "    config[\"training::epochs_train\"] = 15\n",
    "    config[\"training::val_epochs\"] = 10\n",
    "    config[\"training::output_epoch\"] = 10\n",
    "    config[\"training::idx_out\"] = 10\n",
    "    config[\"training::checkpoint_dir\"] = None\n",
    "    config[\"training::tensorboard_dir\"] = None\n",
    "    config[\"seed\"] = 42\n",
    "    config[\"training::trainloader\"] = trainloader\n",
    "    config[\"training::testloader\"] = testloader\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    MLP = NNmodule(config, cuda=cuda, verbosity=0)\n",
    "    ## start training loop\n",
    "    accs = {k : {'train': 0, 'test': 0, 'val': 0} for k in range(0, config[\"training::epochs_train\"])}\n",
    "    losses = {k : {'train': 0, 'test': 0, 'val': 0} for k in range(0, config[\"training::epochs_train\"])}\n",
    "    \n",
    "    for idx in range(config[\"training::epochs_train\"]):\n",
    "        loss_train, acc_train = MLP.train_epoch(trainloader, epoch=-1)\n",
    "        loss_test, acc_test = MLP.test_epoch(testloader, epoch=-1)\n",
    "        if prop_val:\n",
    "            loss_val, acc_val = MLP.test_epoch(valloader, epoch=-1)\n",
    "        else:\n",
    "            loss_val, acc_val = None, None\n",
    "        accs[idx]['train'] = acc_train\n",
    "        accs[idx]['test'] = acc_test\n",
    "        accs[idx]['val'] = acc_val\n",
    "        losses[idx]['train'] = loss_train\n",
    "        losses[idx]['test'] = loss_test\n",
    "        losses[idx]['val'] = loss_val\n",
    "        if verbose: print(f'{idx} - train: {acc_train*100:3.3f} - val {acc_val*100:3.3f} - test {acc_test*100:3.3f}')\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict binary clean/trig\n",
    "# embeddings\n",
    "losses_embds, accs_embds = predict_poisoned(z_train=z_tr,z_val=z_vl,z_test=z_te,prop_train=p_tr,prop_test=p_te,prop_val=p_vl)\n",
    "# raw weights\n",
    "losses_wts, accs_wts = predict_poisoned(z_train=w_tr,z_val=w_vl,z_test=w_te,prop_train=p_tr,prop_test=p_te,prop_val=p_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4ce98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974b28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d46601bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593012c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
