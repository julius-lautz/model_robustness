{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6cbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook with basic loading tests for dataset classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd5e01",
   "metadata": {},
   "source": [
    "# test new token dataset class with epoch resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f087bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_base\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from typing import Union, List, Tuple, Dict, Any\n",
    "\n",
    "from shrp.datasets.dataset_tokens import DatasetTokens\n",
    "from shrp.datasets.augmentations import WindowCutter\n",
    "\n",
    "from shrp.git_re_basin.git_re_basin import (\n",
    "    PermutationSpec,\n",
    "    resnet18_permutation_spec,\n",
    "    weight_matching,\n",
    "    apply_permutation,\n",
    "    zoo_cnn_permutation_spec,\n",
    ")\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from shrp.datasets.dataset_auxiliaries import tokenize_checkpoint, tokens_to_checkpoint\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9119ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:17:01,904\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:08<00:00,  3.28it/s]\n",
      "14it [00:00, 70.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:48<00:00,  3.44s/it]\n"
     ]
    }
   ],
   "source": [
    "zoo_path = [\n",
    "    Path(\n",
    "        \"/netscratch2/kschuerholt/code/versai/model_zoos/zoos/CIFAR10/resnet19/kaiming_uniform/tune_zoo_cifar10_resnet18_kaiming_uniform\"\n",
    "    ).absolute()\n",
    "]\n",
    "permutation_spec = resnet18_permutation_spec()\n",
    "tokensize = 576 / 2\n",
    "\n",
    "# zoo_path = Path(\"/netscratch2/dtaskiran/zoos/SVHN/tune_zoo_svhn_uniform/\")\n",
    "# permutation_spec=zoo_cnn_permutation_spec()\n",
    "# tokensize = 64\n",
    "\n",
    "epoch_list = [1, 5,]\n",
    "map_to_canonical = False\n",
    "# standardize = True\n",
    "standardize = False\n",
    "ds_split = [0.7, 0.15, 0.15]\n",
    "max_samples = 20\n",
    "weight_threshold = 2500\n",
    "num_threads = 30\n",
    "shuffle_path = True\n",
    "windowsize = 1024\n",
    "supersample = \"auto\"\n",
    "precision = \"32\"\n",
    "ignore_bn = True\n",
    "\n",
    "\n",
    "\n",
    "result_key_list = [\"test_acc\", \"training_iteration\", \"ggap\"]\n",
    "config_key_list = []\n",
    "property_keys = {\n",
    "    \"result_keys\": result_key_list,\n",
    "    \"config_keys\": config_key_list,\n",
    "}\n",
    "\n",
    "dataset = DatasetTokens(\n",
    "        root=zoo_path,\n",
    "        epoch_lst=epoch_list,\n",
    "        permutation_spec=permutation_spec,\n",
    "        map_to_canonical=map_to_canonical,\n",
    "        standardize=standardize,\n",
    "        train_val_test=\"train\",  # determines which dataset split to use\n",
    "        ds_split=ds_split,  #\n",
    "        max_samples=max_samples,\n",
    "        weight_threshold=weight_threshold,\n",
    "        precision=precision,\n",
    "        filter_function=None,  # gets sample path as argument and returns True if model needs to be filtered out\n",
    "        property_keys=property_keys,\n",
    "        num_threads=12,\n",
    "        shuffle_path=True,\n",
    "        verbosity=3,\n",
    "        getitem=\"tokens+props\",\n",
    "        ignore_bn=ignore_bn,\n",
    "        tokensize=tokensize,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a2d59f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "from shrp.datasets.progress_bar import ProgressBar\n",
    "\n",
    "# from shrp.datasets.dataset_tokens import tokenize_checkpoint, tokens_to_checkpoint\n",
    "\n",
    "def precompute_permutations(ref_checkpoint,permutation_number, perm_spec, tokensize, ignore_bn, num_threads=6):\n",
    "        logging.info(\"start precomputing permutations\")\n",
    "        model_curr = ref_checkpoint\n",
    "        # find permutation of model to itself as reference\n",
    "        reference_permutation = weight_matching(\n",
    "            ps=perm_spec, params_a=model_curr, params_b=model_curr\n",
    "        )\n",
    "\n",
    "        logging.info(\"get random permutation dicts\")\n",
    "        # compute random permutations\n",
    "        permutation_dicts = []\n",
    "        for ndx in range(permutation_number):\n",
    "            perm = copy.deepcopy(reference_permutation)\n",
    "            for key in perm.keys():\n",
    "                # get permuted indecs for current layer\n",
    "                perm[key] = torch.randperm(perm[key].shape[0]).float()\n",
    "            # append to list of permutation dicts\n",
    "            permutation_dicts.append(perm)\n",
    "\n",
    "\n",
    "        logging.info(\"get permutation indices\")\n",
    "        \"\"\"\n",
    "        1: create reference tokenized checkpoints with two position indices\n",
    "        - position of token in the sequence\n",
    "        - position of values within the token (per token)\n",
    "        \n",
    "        2: map those back to checkpoints\n",
    "        \n",
    "        3: apply permutations on checkpoints\n",
    "        \n",
    "        4: tokenize the permuted checkpoints again\n",
    "        \n",
    "        (5: at getitem: apply permutation on tokenized checkpoints)\n",
    "        \n",
    "        \"\"\"\n",
    "        #1: get reference checkpoint\n",
    "        ref_checkpoint = copy.deepcopy(model_curr)\n",
    "        ## tokenize reference\n",
    "        ref_tok_global, positions = tokenize_checkpoint(\n",
    "            checkpoint=ref_checkpoint, tokensize = tokensize, return_mask=False, ignore_bn=ignore_bn,\n",
    "            )\n",
    "\n",
    "        seqlen, tokensize = ref_tok_global.shape[0],ref_tok_global.shape[1]\n",
    "        \n",
    "        # global index on flattened vector \n",
    "        ref_tok_global = torch.arange(seqlen*tokensize)\n",
    "        \n",
    "        # view in original shape\n",
    "        ref_tok_global = ref_tok_global.view(seqlen,tokensize)\n",
    "        print(ref_tok_global.shape)\n",
    "\n",
    "        # 2: map reference positions \n",
    "        ref_checkpoint_global = tokens_to_checkpoint(\n",
    "            tokens=ref_tok_global, \n",
    "            pos=positions, \n",
    "            reference_checkpoint=ref_checkpoint, ignore_bn=ignore_bn\n",
    "        )\n",
    "                                             \n",
    "        # 3: apply permutations on checkpoints\n",
    "        ray.init(num_cpus=num_threads)\n",
    "        pb = ProgressBar(total=permutation_number)\n",
    "        pb_actor = pb.actor\n",
    "        # get permutations\n",
    "        permutations_global = []\n",
    "        for perm_dict in permutation_dicts:\n",
    "            perm_curr_global = compute_single_perm.remote(\n",
    "                reference_checkpoint=ref_checkpoint_global,\n",
    "                permutation_dict=perm_dict,\n",
    "                perm_spec=perm_spec,\n",
    "                tokensize=tokensize, \n",
    "                ignore_bn=ignore_bn,\n",
    "                pba=pb_actor,\n",
    "            )\n",
    "                        \n",
    "            permutations_global.append(perm_curr_global)\n",
    "\n",
    "        permutations_global = ray.get(permutations_global)\n",
    "                \n",
    "        ray.shutdown()\n",
    "\n",
    "        # cast to torch.int\n",
    "        permutations_global = [perm_g.to(torch.int) for perm_g in permutations_global]\n",
    "\n",
    "        \n",
    "        return permutations_global, permutation_dicts\n",
    "    \n",
    "@ray.remote(num_returns=1)\n",
    "def compute_single_perm(reference_checkpoint, permutation_dict, perm_spec, tokensize, ignore_bn, pba):\n",
    "    # copy reference checkpoint\n",
    "    index_check = copy.deepcopy(reference_checkpoint)\n",
    "    # apply permutation on checkpoint\n",
    "    index_check_perm = apply_permutation(\n",
    "        ps=perm_spec, perm=permutation_dict, params=index_check\n",
    "    )\n",
    "    # vectorize\n",
    "    index_perm, _ = tokenize_checkpoint(\n",
    "            checkpoint=index_check_perm, tokensize = tokensize, return_mask=False, ignore_bn=ignore_bn,\n",
    "            )\n",
    "    # update counter\n",
    "    pba.update.remote(1)\n",
    "    # return list\n",
    "    return index_perm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b9efccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_checkpoint(tokens, pos, reference_checkpoint, ignore_bn=False):\n",
    "    \"\"\"\n",
    "    casts sequence of tokens back to checkpoint\n",
    "    Args:\n",
    "        tokens: sequence of tokens\n",
    "        pos: sequence of positions\n",
    "        reference_checkpoint: reference checkpoint to be used for shape information\n",
    "        ignore_bn: bool wether to ignore batchnorm layers\n",
    "    Returns\n",
    "        checkpoint: checkpoint with weights and biases\n",
    "    \"\"\"\n",
    "    # make copy to prevent memory management issues\n",
    "    checkpoint = copy.deepcopy(reference_checkpoint)\n",
    "    # use only weights and biases\n",
    "    idx = 0\n",
    "    for key in checkpoint.keys():\n",
    "        if \"weight\" in key:\n",
    "            # get correct slice of modules out of vec sequence\n",
    "            if ignore_bn and (\"bn\" in key or \"downsample.1\" in key):\n",
    "                continue\n",
    "\n",
    "            # get modules shape\n",
    "            mod_shape = checkpoint[key].shape\n",
    "\n",
    "            # get slice for current layer\n",
    "            idx_channel = torch.where(pos[:, 1] == idx)[0]\n",
    "            w_t = torch.index_select(input=tokens, index=idx_channel, dim=0)\n",
    "\n",
    "            # infer length of content\n",
    "            contentlength = int(torch.prod(torch.tensor(mod_shape)) / mod_shape[0])\n",
    "\n",
    "            # update weights\n",
    "            try:\n",
    "                # recast to output channels match, padding at the end\n",
    "#                 w_t = w_t.view(mod_shape[0],-1)\n",
    "                checkpoint[key] = w_t.view(mod_shape[0],-1)[:, :contentlength].view(mod_shape)\n",
    "            except Exception as e:\n",
    "                print(f'error matching layer {idx} {key}')\n",
    "                print(e)\n",
    "\n",
    "            # check for bias\n",
    "            try:\n",
    "                if key.replace(\"weight\", \"bias\") in checkpoint:\n",
    "                    checkpoint[key.replace(\"weight\", \"bias\")] = w_t.view(mod_shape[0],-1)[:, contentlength]\n",
    "            except Exception as e:\n",
    "                print(f'bias error matching layer {idx} {key}')\n",
    "                print(e)\n",
    "\n",
    "            # update counter\n",
    "            idx += 1\n",
    "\n",
    "    return checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0d8bdf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_model_vector(wdx,idx_start,window,perm):\n",
    "\n",
    "    # get slice window on token sequence\n",
    "    idx_end = idx_start + window\n",
    "    tokensize = perm.shape[1]\n",
    "    \n",
    "    # slice permutation in tokenized shape\n",
    "    perm = perm[idx_start:idx_end]\n",
    "    \n",
    "    # flatten perms\n",
    "    perm = perm.view(-1)\n",
    "    \n",
    "    # slice permutation out of flattened weight tokens\n",
    "    wdx = torch.index_select(input=wdx.view(-1), index=perm, dim=0)\n",
    "\n",
    "    # reshape weights\n",
    "    wdx = wdx.view(window,tokensize)\n",
    "    \n",
    "    # return tokens\n",
    "    return wdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e17b14",
   "metadata": {},
   "source": [
    "# Check equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4c974643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43924, 288])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 15:54:08,930\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "# permutation_spec = resnet18_permutation_spec(batchnorm=True)\n",
    "perms, permdicts = precompute_permutations(\n",
    "    ref_checkpoint=dataset.reference_checkpoint,\n",
    "    permutation_number=3, \n",
    "    tokensize=tokensize,\n",
    "    ignore_bn=False,\n",
    "    perm_spec=permutation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4314fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that permuted checkpoint == token_to_checkpoint(perm(tokenize(check)))\n",
    "\n",
    "check = copy.deepcopy(dataset.reference_checkpoint)\n",
    "\n",
    "check_p = apply_permutation(\n",
    "        ps=permutation_spec, perm=permdicts[0], params=copy.deepcopy(check)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f1526cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.num_batches_tracked\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.num_batches_tracked\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.num_batches_tracked\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.num_batches_tracked\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.num_batches_tracked\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.num_batches_tracked\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.0.downsample.1.running_mean\n",
      "layer2.0.downsample.1.running_var\n",
      "layer2.0.downsample.1.num_batches_tracked\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.num_batches_tracked\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.num_batches_tracked\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.num_batches_tracked\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.num_batches_tracked\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.0.downsample.1.running_mean\n",
      "layer3.0.downsample.1.running_var\n",
      "layer3.0.downsample.1.num_batches_tracked\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.num_batches_tracked\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.num_batches_tracked\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.num_batches_tracked\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.num_batches_tracked\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.0.downsample.1.running_mean\n",
      "layer4.0.downsample.1.running_var\n",
      "layer4.0.downsample.1.num_batches_tracked\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.num_batches_tracked\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.num_batches_tracked\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "# add noise to checkpoint\n",
    "check_n = copy.deepcopy(check)\n",
    "for key in check_n.keys():\n",
    "    print(key)\n",
    "    if \"weight\" in key:\n",
    "        #### get weights ####\n",
    "        if ignore_bn and (\"bn\" in key or \"downsample.1\" in key):\n",
    "            continue\n",
    "    check_n[key] = torch.randn(check_n[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f2e721f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43924, 288])\n",
      "torch.Size([288])\n",
      "torch.Size([288])\n"
     ]
    }
   ],
   "source": [
    "wdx, pos = tokenize_checkpoint(\n",
    "            checkpoint=copy.deepcopy(check), tokensize = tokensize, return_mask=False, ignore_bn=False,\n",
    "            )\n",
    "perm1 = perms[0]\n",
    "print(wdx.shape)\n",
    "print(perm1[0].shape)\n",
    "print(perm1[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "14fa3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43924, 288])\n"
     ]
    }
   ],
   "source": [
    "wdx2 = permute_model_vector(wdx=wdx,idx_start=0,window=wdx.shape[0],perm=perm1)\n",
    "# wdx2 = torch.stack(wdx2, dim=0)\n",
    "print(wdx2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e7078766",
   "metadata": {},
   "outputs": [],
   "source": [
    "check2 = tokens_to_checkpoint(tokens=wdx2, pos=pos, reference_checkpoint=check_n, ignore_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3a109de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check2.keys() == check.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "91780aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(wdx,wdx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b0a89d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all keys match\n"
     ]
    }
   ],
   "source": [
    "allgood=True\n",
    "for key in check2.keys():\n",
    "    if ignore_bn and (\"bn\" in key or \"downsample.1\" in key):\n",
    "        continue\n",
    "\n",
    "    if not torch.allclose(check2[key],check_p[key]):\n",
    "        print(f'missmatch at {key}')\n",
    "#         print(f'orig: \\t{check[key][:2,]}')\n",
    "#         print(f'noise: \\t{check_n[key][:10,]}')\n",
    "#         print(f'check: \\t{check_p[key][:2]}')\n",
    "#         print(f'token: \\t{check2[key][:2]}')\n",
    "        allgood=False\n",
    "if allgood:\n",
    "    print('all keys match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "741058fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 3, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(n=10, dtype=torch.int32, device='cpu')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd124d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e32f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39124, 288])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:18:39,704\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from shrp.datasets.augmentations import PermutationAugmentation\n",
    "\n",
    "permaug = PermutationAugmentation(\n",
    "    ref_checkpoint=dataset.reference_checkpoint,\n",
    "    permutation_number=100,\n",
    "    perm_spec=permutation_spec,\n",
    "    tokensize=tokensize,\n",
    "    ignore_bn=True,\n",
    "    windowsize=128,\n",
    "    permutations_per_sample = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e860f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transforms = permaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2015dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 288])\n",
      "torch.Size([128, 288])\n",
      "torch.Size([128, 3])\n",
      "tensor([ 0.4363,  1.0000, -0.1091])\n"
     ]
    }
   ],
   "source": [
    "ddx, mask, pos, props = dataset.__getitem__(0)\n",
    "print(ddx.shape)\n",
    "print(mask.shape)\n",
    "print(pos.shape)\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed0bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transforms.perms_per_sample = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f545d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 288])\n",
      "torch.Size([128, 288])\n",
      "torch.Size([128, 3])\n",
      "tensor([ 0.4363,  1.0000, -0.1091])\n"
     ]
    }
   ],
   "source": [
    "ddx, mask, pos, props = dataset.__getitem__(0)\n",
    "print(ddx.shape)\n",
    "print(mask.shape)\n",
    "print(pos.shape)\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "559dcf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9 µs ± 30.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "idx = torch.randint(len(dataset),size=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f767977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.6 ms ± 6.9 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "idx = torch.randint(len(dataset),size=(1,))\n",
    "ddx, mask, pos, props = dataset.__getitem__(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build multi-view augmentation to split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d38bb882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2, dtype=torch.int8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_ids = torch.randperm(\n",
    "            n=ddx.shape[0], dtype=torch.int8, device=ddx.device\n",
    "        )[:1]\n",
    "perm_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68191ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrp.datasets.augmentations import PermutationSelector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9369a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "wdx = torch.randn(128,64)\n",
    "mdx = torch.randn(128,64)\n",
    "pdx = torch.randn(128,3)\n",
    "props = torch.randn(1,4)\n",
    "\n",
    "print(wdx.shape)\n",
    "ps = PermutationSelector(mode=\"canonical\")\n",
    "wdx, mdx, pdx, props = ps(wdx, mdx, pdx, props)\n",
    "print(wdx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9674144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "wdx = torch.randn(128,64)\n",
    "mdx = torch.randn(128,64)\n",
    "pdx = torch.randn(128,3)\n",
    "props = torch.randn(1,4)\n",
    "\n",
    "print(wdx.shape)\n",
    "ps = PermutationSelector(mode=\"identity\")\n",
    "wdx, mdx, pdx, props = ps(wdx, mdx, pdx, props)\n",
    "print(wdx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3acd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (wdx, mdx, pdx, props)\n",
    "wdx, mdx, pdx, props = ps2(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ffa98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0775, -0.4073, -0.2606,  0.1260,  0.1148, -0.8796, -0.5024,  0.6747,\n",
       "        -0.0297, -1.8589,  1.6551,  0.1600, -2.0293,  0.3282, -0.5994, -0.6216,\n",
       "         1.4658, -0.9540, -0.6741, -1.2107,  0.2330, -0.3016,  0.0740,  1.1781,\n",
       "         0.3280, -1.1359,  0.8650, -0.1146,  0.8067,  0.0961,  0.2878, -0.9106,\n",
       "        -0.2778, -1.3343, -0.0446,  0.4382,  0.3909, -0.3165, -0.8315, -0.7822,\n",
       "         2.0207,  1.7968,  0.7648,  0.3996, -0.8529, -0.1022,  1.2889, -0.4179,\n",
       "         0.6985, -0.4624,  1.0634, -0.4380, -0.1787, -0.5775, -1.0185, -1.3324,\n",
       "         0.7661, -1.6492, -1.7587, -0.4814, -1.7970, -1.2670, -1.0457, -0.6354])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91d28ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.62935662007961"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12976128\n",
    "import math\n",
    "math.log2(12976128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfd28aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16777216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea148b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8388608"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*1 << 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c2a27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log2(8388608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "12976128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
